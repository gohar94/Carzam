{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture:\n",
    "**Vgg16** is built on top of **Keras** which sits on top of **Theano** (in our case but it can also be Tensorflow) which in turn sits on top of **CUDA** (cuDNN.)\n",
    "\n",
    "### Note:\n",
    "1) To use Tensorflow as the Keras backend instead of Theano (for using multiple GPUs to achieve higher accuracy later on) change the configurations in *~/.keras/keras.json*\n",
    "\n",
    "2) For switching between GPU (if a GPU is available) and CPU usage in Theano change the configuration in *~/.theanorc*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "INFO (theano.gof.compilelock): Waiting for existing lock by process '18085' (I am process '20779')\n",
      "INFO (theano.gof.compilelock): To manually release the lock, delete /home/ubuntu/.theano/compiledir_Linux-4.4--generic-x86_64-with-debian-stretch-sid-x86_64-2.7.12-64/lock_dir\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing import image\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.random import permutation\n",
    "np.set_printoptions(precision=4, linewidth=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Contains some utilty functions\n",
    "# TODO Figure out what \"reload\" does\n",
    "import utils; reload(utils)\n",
    "from utils import plots\n",
    "# Contains the trained Vgg16 model (2014 winner of ImageNet)\n",
    "import vgg16; reload(vgg16)\n",
    "from vgg16 import Vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For showing the plots in this webpage\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 2] No such file or directory: '../data/dogscats/sample/models/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-2ebcb80b3101>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Path containing trained models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'models/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m: [Errno 2] No such file or directory: '../data/dogscats/sample/models/'"
     ]
    }
   ],
   "source": [
    "# Path containing input data items\n",
    "# path = \"../data/dogscats/\"\n",
    "path = \"../data/dogscats/sample/\"\n",
    "# Path containing trained models\n",
    "model_path = path + 'models/'\n",
    "if not os.path.exists(model_path): os.mkdir(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No larger than 64 batch size is recommended. \n",
    "\n",
    "The idea of a batch is to keep most (ideally all) of the GPU busy processing images; using just one image at a time is not fully utilizing all the cores in the GPU but using the entire data set is also not possible due to the memory constraints on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If running out of memory or using an older GPU, decrease this number\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vgg = Vgg16()\n",
    "# Grab a few images at a time for training and validation\n",
    "batches = vgg.get_batches(path+'train', batch_size=batch_size)\n",
    "val_batches = vgg.get_batches(path+'valid', batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can first see a batch of images and the labels images in this batch have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imgs, labels = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plots(imgs, titles=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use the same images and ignore their original labels and make the pre-trained Vgg16 model classify these images (to see what the model thinks these images are.)\n",
    "\n",
    "The output will contain probabilities, class indices and class names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vgg.predict(imgs, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, at this stage, the classes can be any of the 1000 classes that the Vgg16 model is already trained on. To tailor this model for our needs, we can **finetune** it so that it takes in our training data (from the batches we created above) and only classifies the images using the classes we have labeled our training data with. \n",
    "\n",
    "(The way it identifies which classes the training data can have is by following the directory structure of the *dogscats* data set.)\n",
    "\n",
    "*Note:*\n",
    "In **finetune** we modify the structure of the model to meet our data set - for example, earlier, the output layer would have 1000 categories but now we would only need 2. Such things are done in the finetune function.\n",
    "In **fit** we compute the new weights of the model and the actual re-training happens here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg.finetune(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we **fit** the parameters of the model using training data and the accuracy is reported on the validation set after each epoch. \n",
    "\n",
    "(An epoch is one full pass through the training data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vgg.fit(batches, val_batches, nb_epoch=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vgg.model.save_weights(model_path+'finetune1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Tip for debugging:**\n",
    "\n",
    "It is a good idea to look at examples of each of:\n",
    "1. A few correct labels at random\n",
    "2. A few incorrect labels at random\n",
    "3. The most correct labels of each class (i.e. those with highest probability that are correct)\n",
    "4. The most incorrect labels of each class (i.e. those with highest probability that are incorrect)\n",
    "5. The most uncertain labels (i.e. those with probability closest to 0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vgg **test** function is a utility function that first calls get_batches on the provided path and on those batches, it runs the prediction and puts all the results into one array and returns that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the validation set instead of the test set because we know the correct answers to the validation set so that will help us in debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_batches, probs = vgg.test(path+'valid', batch_size=batch_size)\n",
    "labels = val_batches.classes\n",
    "filenames = val_batches.filenames\n",
    "# Pick the first column, which in this case would represent probability of the image being a cat\n",
    "probs = probs[:,0]  \n",
    "# If the probability of it being a cat is high then assign class label 1 else assign class label 0\n",
    "preds = np.round(1-probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probs[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of images to view for each category\n",
    "n_view = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plots_idx(idx, titles=None):\n",
    "    \"\"\"Helper function for plotting images by index in the validation set.\"\"\"\n",
    "    if len(idx) > 0:\n",
    "        plots([image.load_img(path+'valid/'+filenames[i]) for i in idx], titles=titles)\n",
    "    else:\n",
    "        print \"Nothing to plot!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1. A few correct labels at random\n",
    "correct = np.where(preds==labels)[0]\n",
    "idx = permutation(correct)[:n_view]\n",
    "plots_idx(idx, preds[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2. A few incorrect labels at random\n",
    "incorrect = np.where(preds!=labels)[0]\n",
    "idx = permutation(incorrect)[:n_view]\n",
    "plots_idx(idx, preds[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 3. The images we were most confident were class 0, and are actually class 1\n",
    "correct_class_0 = np.where((preds==0) & (preds!=labels))[0]\n",
    "idx = np.argsort(probs[correct_class_0])[::-1][:n_view]\n",
    "plots_idx(idx, preds[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 3. The images we were most confident were class 1, and are actually class 1\n",
    "correct_class_1 = np.where((preds==1) & (preds!=labels))[0]\n",
    "idx = np.argsort(probs[correct_class_1])[::-1][:n_view]\n",
    "plots_idx(idx, preds[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 4. The images we were most confident were class 0, but are actually class 1\n",
    "incorrect_class_0 = np.where((preds==0) & (preds!=labels))[0]\n",
    "idx = permutation(incorrect_class_0)[:n_view]\n",
    "plots_idx(idx, preds[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 4. The images we were most confident were class 1, but are actually class 0\n",
    "incorrect_class_1 = np.where((preds==1) & (preds!=labels))[0]\n",
    "idx = permutation(incorrect_class_1)[:n_view]\n",
    "plots_idx(idx, preds[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 5. The most uncertain labels (ie those with probability closest to 0.5)\n",
    "most_uncertain = np.argsort(np.abs(probs-0.5))\n",
    "plots_idx(most_uncertain[:n_view], probs[most_uncertain])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
